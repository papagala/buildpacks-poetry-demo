{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cde6d7",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before you run this notebook you should make sure that you use the appropriate kernel.\n",
    "\n",
    "You can create the kernel using the repo's conda environment.\n",
    "\n",
    "For instructions on how to setup your conda/poetry environment and create your kernel, check the documentation in  our [wiki](https://code.roche.com/one-d-ai/early-adopters/user-guide-wiki/-/wikis/How-To/sHPC-setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e88297",
   "metadata": {},
   "source": [
    "# Import the library\n",
    "\n",
    "When you run the notebook inside the repo you import the code like a package.\n",
    "\n",
    "The only file we will need from this package is the `functions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1764e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_diabetes_example import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dde7f3",
   "metadata": {},
   "source": [
    "# Use the model\n",
    "\n",
    "## Load model\n",
    "\n",
    "- First, we need to load the model inside the notebook using the `load_model()` funtion, where we pass as parameters:\n",
    "    - the path where the model output was saved after training\n",
    "    - any metadata we want to pass to the model we call _(this is optional)_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada4d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Path package here to properly define paths\n",
    "from pathlib import Path\n",
    "repo_path = Path().absolute().parent.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af8e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the path and name of your model\n",
    "model_path = Path(repo_path, \"model\")\n",
    "my_model_name = \"sklearn-diabetes-model-new-version.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de207809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = functions.load_model(model_path, my_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb85c5e",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "- Then, we can call the model by using the `inference()` funtion, where we pass as parameters:\n",
    "    - the model, which is the object the `load_model()` funtion returned above\n",
    "    - the data we want to pass to the model\n",
    "    - any metadata we want to pass to the model we call _(this is optional)_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516ab43",
   "metadata": {},
   "source": [
    "_**Important note:**_ In order to know the type of data the model expects as input, we need to either check the code of the `inference()` funtion inside the `functions.py` file or in principle get this information from the model owner _(the person who knows how the model was created)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a100db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is already a sample data file available for you to use as an example\n",
    "data_filepath = Path(repo_path, \"data/sample-inference-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e1164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our model we accept a pandas DataFrame as input\n",
    "# this might not be the best choise of a data file format however \n",
    "# if we want to create workflows where the End Users provide us with a data file as input\n",
    "import pandas as pd\n",
    "data = pd.read_csv(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358f710f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [194.6992056369373,\n",
       "  152.6980131720448,\n",
       "  172.39786672205219,\n",
       "  110.29996319382778,\n",
       "  164.44916042190607,\n",
       "  133.0813467209344,\n",
       "  259.08127516971854,\n",
       "  100.10086165145408,\n",
       "  116.83706204837088,\n",
       "  123.2848780836174,\n",
       "  219.30543578124906,\n",
       "  63.584473095152134,\n",
       "  134.03249073071373,\n",
       "  120.58477063277871,\n",
       "  53.694089524808604,\n",
       "  190.6169607158408,\n",
       "  105.77502959009544,\n",
       "  124.80817601462425,\n",
       "  207.80631977953618,\n",
       "  54.48294121058534]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in our case the data output will be in a dictionary format\n",
    "# this is a helpful format when our model is callable from a REST API\n",
    "# because then we can use a json file type to save the results and parse them elsewhere\n",
    "functions.inference(model, data, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0213e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
